from cgi import test
import os
import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms

from torch.utils.tensorboard import SummaryWriter


dataset_path = 'c:/Users/night/Documents/09/school/actual-masters/git/masters/models/data/fma/classified_small'

batch_size = 32  # 64 for gpu
img_height = 235
img_width = 352

epochs = 20 # 50
test_split = 0.8


transform = transforms.Compose([
            transforms.Resize([img_height, img_width]), # Resizing the image as the VGG only take 224 x 244 as input size
            transforms.RandomHorizontalFlip(), # Flip the data horizontally
            transforms.ToTensor(),
            transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5))])

data = datasets.ImageFolder(root=dataset_path, transform=transform)
print(len(data))

test_split_size = int(len(data)*test_split)
print(test_split_size)
testset, valset = torch.utils.data.random_split(data, (test_split_size, len(data)-test_split_size))

test_ds = torch.utils.data.DataLoader(dataset=testset, batch_size=batch_size, shuffle=True)
validation_ds = torch.utils.data.DataLoader(dataset=valset, batch_size=batch_size, shuffle=True)


class Net(nn.Module):
    def __init__(self):
        
        super().__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=2)  # 3 input bc rgb?
        self.conv2 = nn.Conv2d(32, 64, 3, padding=2)
        self.conv3 = nn.Conv2d(64, 128, 3, padding=2)

        self.fc1 = nn.Linear(32, 128)
        self.fc2 = nn.Linear(128, 8)
        # self.fc3 = nn.Linear(128, 8)

        self.dropout = nn.Dropout2d(0.2) # do we need to add this in? eh why not

    def convs(self, x):
        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))
        x = F.max_pool2d(F.relu(self.conv2(x)), (2,2))
        x = F.max_pool2d(F.relu(self.conv3(x)), (2,2))
        print(x.size(0))

        x = self.dropout(x)

        return x

    def forward(self, x):
        x = self.convs(x)
        print("x size:", x.size(0))
        x = x.view(x.size(0), -1) # i still don't get the -1 but for later
        print(x.size(0))

        x = F.relu(self.fc1(x)) 
        # x = F.relu(self.fc2(x))
        x = self.fc2(x)

        return F.softmax(x, dim=1)

net = Net()



optimiser = optim.Adam(net.parameters(), lr=0.005)


for i in range(epochs):
    for data in test_ds:
        print(len(data))
        # batch_x, batch_y = next(iter(data_loader))
        # print(np.shape(batch_x), batch_y)
        X, y = data
        print(X.shape, y)
        net.zero_grad()
        output = net.forward(X)
        loss = F.nll_loss(output, y)
        loss.backward()
        optimiser.step()
    print(loss)


correct = 0
total = 0

# data_loader = get_data_loader(test=False)
with torch.no_grad():
        X, y = validation_ds
        output = net.forward(X)
        for idx, i in enumerate(output):
            if torch.argmax(i) == y[idx]:
                correct += 1
            total += 1
print("accuracy:", round(correct/total, 3))